{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVS1JrCRGQZ1",
        "outputId": "d1737503-a7cb-49da-d4c1-91c95fb041f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium highway-env stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OebvqYsR91Pt",
        "outputId": "ee60ca38-88e9-48cf-b76b-badf2c0c5f9a"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "# import highway_env\n",
        "# highway_env.register_highway_envs()\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "from tqdm.notebook import tqdm\n",
        "import itertools\n",
        "import pickle\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# from stable_baselines3 import DQN\n",
        "# from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "# from stable_baselines3.common.monitor import Monitor\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
        "from keras.callbacks import TensorBoard\n",
        "from keras.optimizers import Adam\n",
        "from scipy.signal import convolve, gaussian\n",
        "import tensorflow as tf\n",
        "\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def timeit(f):\n",
        "\n",
        "    def timed(*args, **kw):\n",
        "\n",
        "        ts = time.time()\n",
        "        result = f(*args, **kw)\n",
        "        te = time.time()\n",
        "\n",
        "        print('func:%r took: %2.4f sec' % \\\n",
        "          (f.__name__, te-ts))\n",
        "        return result\n",
        "\n",
        "    return timed\n",
        "\n",
        "action_names = ['left', 'idle', 'right', 'faster', 'slower']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gdrhVZwR-Uxz"
      },
      "outputs": [],
      "source": [
        "default_config = {\n",
        "    \"lanes_count\" : 4,\n",
        "    \"vehicles_count\": 50,\n",
        "    \"duration\": 40,\n",
        "    \"other_vehicles_type\": \"highway_env.vehicle.behavior.IDMVehicle\",\n",
        "    \"initial_spacing\": 2,\n",
        "    \"simulation_frequency\": 15,\n",
        "    \"policy_frequency\": 5,\n",
        "    \"collision_reward\": -200,\n",
        "    \"right_lane_reward\": 5,\n",
        "    \"high_speed_reward\": 20,\n",
        "    \"lane_change_reward\": 3,\n",
        "    \"reward_speed_range\": [20, 30],\n",
        "    \"normalize_reward\": False,\n",
        "    \"screen_width\": 800,\n",
        "    \"screen_height\": 600,\n",
        "    \"centering_position\": [0.5, 0.5],\n",
        "    \"scaling\": 5,\n",
        "    \"show_trajectories\": True,\n",
        "    \"render_agent\": True,\n",
        "    \"offscreen_rendering\": False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeToCollisionVec:\n",
        "    def __init__(self, \n",
        "                num_envs=1, \n",
        "                horizon=5,\n",
        "                policy_frequency=5,\n",
        "                simulation_frequency=15,\n",
        "                collision_reward=-200,\n",
        "                high_speed_reward=10,\n",
        "                right_lane_reward=10,\n",
        "                lane_change_reward=-1,\n",
        "                obs_type='speed-restricted',\n",
        "                string_shaped=True,\n",
        "                ):\n",
        "        \"\"\"\n",
        "        Constructor for the TimeToCollisionVec class.\n",
        "        Args:\n",
        "            num_envs: Number of parallel environments to run.\n",
        "            horizon: The number of timesteps to look ahead for the TimeToCollision observation.\n",
        "            policy_frequency: The frequency at which the policy is evaluated.\n",
        "            simulation_frequency: The frequency at which the simulation is run.\n",
        "            high_speed_reward: The reward given for driving at high speed.\n",
        "            right_lane_reward: The reward given for driving in the right lane.\n",
        "            lane_change_reward: The reward given for changing lanes.\n",
        "            obs_type: The type of observation to use. Choose from 'normal' or 'speed-restricted'\n",
        "                'normal': The normal TimeToCollision observation (x3 different speeds the car could be at)\n",
        "                'speed-restricted': Only one of the TimeToCollision observations is used (the speed the car is currently at) (less complex observation space)\n",
        "            string_shaped: Whether to return the observation as a string shape (num_envs, dim1*dim2*dim3*...) or not (num_envs, dim1, dim2, dim3, ...)\n",
        "        \"\"\"\n",
        "        assert obs_type in ['normal', 'speed-restricted'], \"obs_type must be either 'normal' or 'speed-restricted'\"\n",
        "        if obs_type == 'speed-restricted' and not string_shaped:\n",
        "            raise ValueError(\"If obs_type is 'speed-restricted', string_shaped must be True\")\n",
        "        \n",
        "        self.config = default_config.copy()\n",
        "        self.config[\"observation\"] =  {\n",
        "                \"type\": \"TimeToCollision\",\n",
        "                \"horizon\": horizon}\n",
        "        self.horizon = horizon\n",
        "\n",
        "        self.config.update({\n",
        "            \"policy_frequency\": policy_frequency,\n",
        "            \"simulation_frequency\": simulation_frequency,\n",
        "            \"high_speed_reward\": high_speed_reward,\n",
        "            \"right_lane_reward\": right_lane_reward,\n",
        "            \"lane_change_reward\": lane_change_reward,\n",
        "            \"collision_reward\": collision_reward,\n",
        "        })\n",
        "        \n",
        "        self.num_envs = num_envs\n",
        "        self.obs_type = obs_type\n",
        "        self.string_shaped = string_shaped\n",
        "\n",
        "        with gym.make(\"highway-fast-v0\", config=self.config) as env:\n",
        "            self.env = gym.vector.AsyncVectorEnv([lambda: env for _ in range(num_envs)])\n",
        "            self.obs_space_shape = env.observation_space.shape if not obs_type == 'speed-restricted' else (env.observation_space.shape[1:])\n",
        "            self.action_space_size = env.action_space.n\n",
        "\n",
        "    def get_speeds(self):\n",
        "        return [car.speed_index for car in self.env.get_attr('vehicle')]\n",
        "    \n",
        "    def get_state(self, obs):\n",
        "        observation = np.array(obs)\n",
        "        if self.obs_type == 'speed-restricted':\n",
        "            speed_indices = self.get_speeds()\n",
        "            observation =  np.array([observation[i][speed_indices[i]] for i in range(self.num_envs)])\n",
        "        if self.string_shaped:\n",
        "            # Convert the observation to a string shape: from (num_envs, dim1, dim2, dim3, ...) to (num_envs, dim1*dim2*dim3*...)\n",
        "            observation = observation.reshape((self.num_envs, -1))\n",
        "        return observation\n",
        "    \n",
        "    def reset(self, seeds=None):\n",
        "        if seeds is None:\n",
        "            seeds = [np.random.randint(100000) for _ in range(self.num_envs)]\n",
        "        obs, info = self.env.reset(seed=seeds)\n",
        "        return self.get_state(obs)\n",
        "        \n",
        "    def step(self, actions, string_shaped=False):\n",
        "        obs, rewards, dones, truncates, info = self.env.step(actions)\n",
        "        return self.get_state(obs), np.array(rewards), np.array(dones), np.array(truncates)\n",
        "    \n",
        "    def close(self):\n",
        "        self.env.close()    \n",
        "\n",
        "    def action_space_sample(self):  \n",
        "        return self.env.action_space.sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TimeToCollision:\n",
        "    def __init__(self, \n",
        "                horizon=5,\n",
        "                policy_frequency=5,\n",
        "                simulation_frequency=15,\n",
        "                high_speed_reward=10,\n",
        "                right_lane_reward=10,\n",
        "                lane_change_reward=-1,\n",
        "                render_mode=None,\n",
        "                obs_type='speed-restricted', \n",
        "                string_shaped=True):\n",
        "        \n",
        "        assert obs_type in ['normal', 'speed-restricted'], \"obs_type must be either 'normal' or 'speed-restricted'\"\n",
        "        if obs_type == 'speed-restricted' and not string_shaped:\n",
        "            raise ValueError(\"If obs_type is 'speed-restricted', string_shaped must be True\")\n",
        "\n",
        "        self.config = default_config.copy()\n",
        "        self.config[\"observation\"] =  {\n",
        "                \"type\": \"TimeToCollision\",\n",
        "                \"horizon\": horizon}\n",
        "        self.horizon = horizon\n",
        "        self.obs_type = obs_type\n",
        "        self.string_shaped = string_shaped\n",
        "\n",
        "        self.config.update({\n",
        "            \"policy_frequency\": policy_frequency,\n",
        "            \"simulation_frequency\": simulation_frequency,\n",
        "            \"high_speed_reward\": high_speed_reward,\n",
        "            \"right_lane_reward\": right_lane_reward,\n",
        "            \"lane_change_reward\": lane_change_reward\n",
        "        })\n",
        "\n",
        "        with gym.make(\"highway-fast-v0\", config=self.config, render_mode=render_mode) as env:\n",
        "            self.env = env\n",
        "            self.obs_space_shape = env.observation_space.shape if not string_shaped else env.observation_space.shape[1:]\n",
        "            self.action_space_size = env.action_space.n\n",
        "\n",
        "    def get_speed(self):\n",
        "        return self.env.get_wrapper_attr('vehicle').speed_index\n",
        "\n",
        "    def get_state(self, obs):\n",
        "        observation = np.array(obs)\n",
        "        if self.obs_type == 'speed-restricted':\n",
        "            speed_index = self.get_speed()\n",
        "            observation = observation[speed_index]\n",
        "        if self.string_shaped:\n",
        "            observation = observation.flatten()\n",
        "        return observation\n",
        "\n",
        "    def reset(self, seed='random'):\n",
        "        if seed == 'random':\n",
        "            seed = np.random.randint(100000)\n",
        "        obs, info = self.env.reset(seed=seed)\n",
        "        return self.get_state(obs)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        return self.get_state(obs), reward, done, truncated\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "    def action_space_sample(self):\n",
        "        return self.env.action_space.sample()\n",
        "\n",
        "    def test_env(self, sleep_time=1):\n",
        "        with gym.make(\"highway-v0\", config=self.config, render_mode='human') as env:\n",
        "            self.env = env\n",
        "            obs = env.reset()\n",
        "            for _ in range(1000):\n",
        "                action = env.action_space.sample()\n",
        "                obs, reward, done, truncated, info = env.step(action)\n",
        "                print(self.get_state(obs), reward)\n",
        "                if done:\n",
        "                    break\n",
        "                time.sleep(sleep_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def CNN(obs_space_shape, action_space_size):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (2, 2), input_shape=obs_space_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(action_space_size, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def CNN_2(obs_space_shape, action_space_size):\n",
        "    init = tf.keras.initializers.HeUniform()\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (2, 2), input_shape=obs_space_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(64))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(action_space_size, activation='linear'))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "def NN_1(obs_space_shape, action_space_size):\n",
        "    init = tf.keras.initializers.HeUniform()\n",
        "    # Make the multiplication in the obs space shape tuple \n",
        "    obs_size = (np.prod(obs_space_shape),)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_shape=obs_size, activation='relu'))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(action_space_size, activation='linear'))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def NN_2(obs_space_shape, action_space_size):\n",
        "    init = tf.keras.initializers.HeUniform()\n",
        "    # Make the multiplication in the obs space shape tuple \n",
        "    obs_size = (np.prod(obs_space_shape),)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_shape=obs_size, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(action_space_size, activation='linear'))\n",
        "    model.compile(loss=tf.keras.losses.Huber(), optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z-XmWnhw-w9A"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, \n",
        "                 env,\n",
        "                 model_type='CNN'):\n",
        "        assert not (model_type in ['CNN','CNN_2'] and (env.obs_type == 'speed-restricted' or env.string_shaped)), 'CNN model cannot be used with speed-restricted observation type'\n",
        "        self.env = env\n",
        "        self.model_type = model_type\n",
        "        self.initialized = False\n",
        "\n",
        "        # Main model - what gets trained every step\n",
        "        self.model = self.create_model()\n",
        "        print('Model Stats:')\n",
        "        print(self.model.summary())\n",
        "        print(self.model.input_shape)\n",
        "\n",
        "        # Target model - what we predict every step\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "        self.target_update_counter = 0\n",
        "\n",
        "    def create_model(self):\n",
        "        if self.model_type == 'CNN':\n",
        "            return CNN(self.env.obs_space_shape, self.env.action_space_size)\n",
        "        elif self.model_type == 'NN':\n",
        "            return NN(self.env.obs_space_shape, self.env.action_space_size)\n",
        "        elif self.model_type == 'CNN_2':\n",
        "            return CNN_2(self.env.obs_space_shape, self.env.action_space_size)\n",
        "        elif self.model_type == 'NN_2':\n",
        "            return NN_2(self.env.obs_space_shape, self.env.action_space_size)\n",
        "        \n",
        "    def init(self, epsilon=1,\n",
        "             min_eps=0.001,\n",
        "             eps_decay=0.998,\n",
        "             gamma=0.99,\n",
        "             alpha=0.75,\n",
        "             replay_memory=50_000,\n",
        "             batch_size=64,\n",
        "             update_weights_freq=100,\n",
        "             train_model_freq=5,\n",
        "             min_replay_memory=1_000\n",
        "             ):\n",
        "        self.initialized = True\n",
        "        self.epsilon, self.min_eps, self.eps_decay = epsilon, min_eps, eps_decay\n",
        "        self.replay_memory = deque(maxlen=replay_memory)\n",
        "        self.update_weights_freq, self.train_model_freq = update_weights_freq, train_model_freq\n",
        "        self.min_replay_memory = min_replay_memory\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma, self.alpha = gamma, alpha\n",
        "        self.current_episode, self.step = 0, 0\n",
        "        self.mean_rw_history, self.mean_steps_history, self.loss_hist, self.action_distribution_history = [], [], [], []\n",
        "\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.extend(transition)\n",
        "\n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(state, verbose=0)\n",
        "\n",
        "    @timeit\n",
        "    def train_model(self, terminal_state):\n",
        "        if len(self.replay_memory) < self.min_replay_memory:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.replay_memory, self.batch_size)\n",
        "\n",
        "        current_states = np.array([transition[0] for transition in minibatch])\n",
        "        # The model that gets trained every step\n",
        "        current_qs_list = self.get_qs(current_states)\n",
        "\n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
        "        # The model that doesn't get trained every step\n",
        "        future_qs_list = self.target_model.predict(new_current_states, verbose=0)\n",
        "\n",
        "        X,y = [], []\n",
        "\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "            new_q = reward\n",
        "\n",
        "            if not done:\n",
        "                # In case of a non-terminal state, add future reward\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q += self.gamma * max_future_q\n",
        "\n",
        "            # Update the q value for the action taken (using the Bellman equation)\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = (1-self.alpha)*current_qs[action] + self.alpha*new_q\n",
        "            \n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "        history = self.model.fit(np.array(X), np.array(y), batch_size=self.batch_size, verbose=0, shuffle=False)\n",
        "\n",
        "        # If counter reaches set value, update target network with weights of main network\n",
        "        if self.step % self.update_weights_freq == 0:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        return history.history['loss'][0] \n",
        "\n",
        "    def epsilon_greedy(self,current_state):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            action = np.argmax(self.get_qs(current_state), axis=1)\n",
        "        else:\n",
        "            # Generate self.env.num_envs random actions\n",
        "            action = self.env.action_space_sample()\n",
        "        return np.array(action)\n",
        "\n",
        "    def decay_eps(self):\n",
        "        if self.epsilon > self.min_eps:\n",
        "            self.epsilon *= self.eps_decay\n",
        "            self.epsilon = max(self.min_eps, self.epsilon)\n",
        "\n",
        "    def train(self, episodes=10_000, episode_duration=200, evaluation_freq=25, tolerance=10):\n",
        "        if not self.initialized:\n",
        "          print('Warning! Initializing with default parameters')\n",
        "          self.init()\n",
        "\n",
        "        start_time = time.time()\n",
        "        for episode in tqdm(range(self.current_episode, episodes), unit='episode'):\n",
        "            current_states = self.env.reset()\n",
        "            self.current_episode += 1\n",
        "            \n",
        "            count = 0\n",
        "            # Episode cycle\n",
        "            while count < episode_duration:\n",
        "                action = self.epsilon_greedy(current_states)\n",
        "                new_states, rewards, done, truncate = self.env.step(action)\n",
        "                done = done | truncate\n",
        "                \n",
        "                to_append = list(zip(current_states, action, rewards, new_states, done))\n",
        "                self.update_replay_memory(to_append)\n",
        "\n",
        "                if self.step % self.train_model_freq == 0:\n",
        "                    loss = self.train_model(False)\n",
        "                    self.loss_hist.append(loss) if loss is not None else None\n",
        "\n",
        "                current_states = new_states\n",
        "                self.step += 1\n",
        "                count += 1\n",
        "\n",
        "            loss = self.train_model(True)\n",
        "            self.loss_hist.append(loss) if loss is not None else None\n",
        "\n",
        "            if self.current_episode % evaluation_freq == 0:\n",
        "                rewards_mean, steps_mean, action_distribution = self.evaluate_and_render(n_games=3, steps=200)\n",
        "                self.mean_rw_history.append(rewards_mean)\n",
        "                self.mean_steps_history.append(steps_mean)\n",
        "                self.action_distribution_history.append(action_distribution)\n",
        "\n",
        "                # If the model is better than the previous best, save it\n",
        "                if len(self.mean_rw_history) > 1 and rewards_mean > max(self.mean_rw_history[:-1]):\n",
        "                    self.model.save(f'models/{self.model_type}_best_{self.current_episode}_{np.random.randint(100000)}.h5')\n",
        "\n",
        "                clear_output(wait=True)\n",
        "\n",
        "                print(f\"episode = {self.current_episode}, epsilon = {self.epsilon}, mean reward = {rewards_mean}, mean steps = {steps_mean}\")\n",
        "                print(f\"total steps = {self.step*self.env.num_envs}, time_elapsed = {time.time() - start_time}, fps = {self.step*self.env.num_envs / (time.time() - start_time)}\")\n",
        "                \n",
        "                plt.figure(figsize=[16, 10])\n",
        "                plt.suptitle(f\"Stats for episode {self.current_episode}\")\n",
        "                plt.subplot(2, 2, 1)\n",
        "                plt.title(\"Mean reward per episode\")\n",
        "                plt.plot(self.mean_rw_history)\n",
        "                plt.grid()\n",
        "\n",
        "                plt.subplot(2, 2, 2)\n",
        "                plt.title(\"Mean steps per episode\")\n",
        "                plt.plot(self.mean_steps_history)\n",
        "                plt.grid()\n",
        "\n",
        "                plt.subplot(2, 2, 3)\n",
        "                plt.title(\"Action distribution for the current episode\")\n",
        "                plt.bar(action_names, action_distribution)\n",
        "                plt.grid()\n",
        "\n",
        "                plt.subplot(2, 2, 4)\n",
        "                plt.title(\"Action distribution history for all episodes\")\n",
        "                action_distribution_history = np.array(self.action_distribution_history)\n",
        "                for i in range(self.env.action_space_size):\n",
        "                    plt.plot(action_distribution_history[:, i], label=f'{action_names[i]}')\n",
        "                plt.legend()\n",
        "                plt.grid()\n",
        "                \n",
        "                plt.show()\n",
        "\n",
        "            # If the 10 last rewards didn't improve at all, stop training\n",
        "            if len(self.mean_rw_history) > tolerance and all([reward <= self.mean_rw_history[0] for reward in self.mean_rw_history[-tolerance:]]):\n",
        "                print(f'Stopping training because the rewards did not improve for the last {tolerance} episodes')\n",
        "                break\n",
        "            self.decay_eps()\n",
        "\n",
        "    def evaluate(self, agent, n_games=3, steps=200, verbose=True):\n",
        "        sim_freq, pol_freq, horizon = self.env.config['simulation_frequency'], self.env.config['policy_frequency'], self.env.horizon\n",
        "        ttc_vec_env = TimeToCollisionVec(num_envs=n_games, horizon=horizon, policy_frequency=pol_freq, simulation_frequency=sim_freq, render_mode=None)\n",
        "\n",
        "        print('Evaluating...') if verbose else None\n",
        "        total_reward, finished_games = 0, 0\n",
        "        obs = ttc_vec_env.reset()\n",
        "        actions_hist = []\n",
        "        for _ in range(steps):\n",
        "            actions = np.argmax(agent.get_qs(obs), axis=1)\n",
        "            actions_hist.extend(actions)\n",
        "            obs, rewards_, dones, _ = ttc_vec_env.step(actions)\n",
        "            print(actions, rewards_)\n",
        "            total_reward += rewards_.sum()\n",
        "            finished_games += dones.sum()\n",
        "\n",
        "        total_steps = steps * n_games\n",
        "        average_reward = total_reward / finished_games if finished_games > 0 else 0\n",
        "        average_steps = total_steps / finished_games if finished_games > 0 else steps\n",
        "        # Calculate the action distribution\n",
        "        actions_hist = np.array(actions_hist)\n",
        "        action_distribution = np.array([np.sum(actions_hist == i) for i in range(ttc_vec_env.action_space_size)]) / len(actions_hist)\n",
        "        ttc_vec_env.close()        \n",
        "        return average_reward, average_steps, action_distribution\n",
        "\n",
        "\n",
        "    def evaluate_and_render(self, n_games=3, steps=300, verbose=True):\n",
        "        sim_freq, pol_freq, horizon = self.env.config['simulation_frequency'], self.env.config['policy_frequency'], self.env.horizon\n",
        "        ttc_env = TimeToCollision(policy_frequency=pol_freq, \n",
        "                                  simulation_frequency=sim_freq, \n",
        "                                  horizon=horizon, \n",
        "                                  render_mode='human')\n",
        "\n",
        "        print('Evaluating...') if verbose else None\n",
        "        rewards, steps_array, actions_hist = [], [], []\n",
        "        for i in tqdm(range(n_games)):\n",
        "            cum_reward = 0\n",
        "            step, done = 0, False\n",
        "            current_state = ttc_env.reset(seed=np.random.randint(100000+i))\n",
        "            while not done and step < steps:\n",
        "                # Get next action\n",
        "                next_action = np.argmax(self.get_qs(current_state.reshape(1, *current_state.shape)))\n",
        "                actions_hist.append(next_action)\n",
        "                current_state, reward, done, truncated = ttc_env.step(next_action)\n",
        "                done = done | truncated\n",
        "                cum_reward += reward\n",
        "                step += 1\n",
        "            rewards.append(cum_reward)\n",
        "            steps_array.append(step)\n",
        "        actions_hist = np.array(actions_hist)\n",
        "        action_distribution = np.array([np.sum(actions_hist == i) for i in range(ttc_env.action_space_size)]) / len(actions_hist)\n",
        "\n",
        "        ttc_env.close()\n",
        "        return np.mean(rewards), np.mean(steps_array), action_distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Stats:\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 128)               5888      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,469\n",
            "Trainable params: 14,469\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "(None, 45)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24b831c777df42e7b52d815a49e39b76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?episode/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "func:'train_model' took: 0.0000 sec\n",
            "func:'train_model' took: 0.0000 sec\n",
            "func:'train_model' took: 0.0000 sec\n",
            "func:'train_model' took: 0.0000 sec\n",
            "func:'train_model' took: 0.0000 sec\n",
            "func:'train_model' took: 0.0000 sec\n",
            "func:'train_model' took: 0.0000 sec\n"
          ]
        }
      ],
      "source": [
        "ttc_vec = TimeToCollisionVec(num_envs=10, \n",
        "                            horizon=5,\n",
        "                            policy_frequency=3, \n",
        "                            simulation_frequency=20,\n",
        "                            collision_reward=-100,\n",
        "                            high_speed_reward=10, \n",
        "                            right_lane_reward=7.5, \n",
        "                            lane_change_reward=0.25,\n",
        "                            obs_type='speed-restricted', \n",
        "                            string_shaped=True)\n",
        "\n",
        "agent = DQNAgent(ttc_vec, model_type='NN_2')\n",
        "agent.init(\n",
        "    epsilon=1,\n",
        "    min_eps=0.01,\n",
        "    eps_decay=0.996,  #.998\n",
        "    gamma=0.99,\n",
        "    alpha=0.3,\n",
        "    replay_memory=50_000,  #50_000\n",
        "    batch_size=64,   # 64\n",
        "    train_model_freq=10,\n",
        "    update_weights_freq=150,\n",
        "    min_replay_memory=1_000, # 1_000\n",
        ")\n",
        "\n",
        "agent.train(episodes=1_000,\n",
        "            episode_duration=50,\n",
        "            evaluation_freq=10,\n",
        "            tolerance=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TimeToCollisionVec' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# MORE REWARD, LESS ALPHA\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ttc_vec_2 \u001b[38;5;241m=\u001b[39m \u001b[43mTimeToCollisionVec\u001b[49m(num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m      3\u001b[0m                             horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      4\u001b[0m                             policy_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \n\u001b[0;32m      5\u001b[0m                             simulation_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      6\u001b[0m                             collision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      7\u001b[0m                             high_speed_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, \n\u001b[0;32m      8\u001b[0m                             right_lane_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m      9\u001b[0m                             lane_change_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.5\u001b[39m,\n\u001b[0;32m     10\u001b[0m                             obs_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeed-restricted\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     11\u001b[0m                             string_shaped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m agent_2 \u001b[38;5;241m=\u001b[39m DQNAgent(ttc_vec_2, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN_2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m agent_2\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m     15\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     16\u001b[0m     min_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     min_replay_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000\u001b[39m, \u001b[38;5;66;03m# 1_000\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'TimeToCollisionVec' is not defined"
          ]
        }
      ],
      "source": [
        "# MORE REWARD, LESS ALPHA\n",
        "ttc_vec_2 = TimeToCollisionVec(num_envs=10, \n",
        "                            horizon=10,\n",
        "                            policy_frequency=3, \n",
        "                            simulation_frequency=20,\n",
        "                            collision_reward=-100,\n",
        "                            high_speed_reward=12, \n",
        "                            right_lane_reward=10, \n",
        "                            lane_change_reward=2.5,\n",
        "                            obs_type='speed-restricted', \n",
        "                            string_shaped=True)\n",
        "\n",
        "agent_2 = DQNAgent(ttc_vec_2, model_type='NN_2')\n",
        "agent_2.init(\n",
        "    epsilon=1,\n",
        "    min_eps=0.01,\n",
        "    eps_decay=0.996,  #.998\n",
        "    gamma=0.99,\n",
        "    alpha=0.10,\n",
        "    replay_memory=50_000,  #50_000\n",
        "    batch_size=64,   # 64\n",
        "    train_model_freq=20,\n",
        "    update_weights_freq=200,\n",
        "    min_replay_memory=1_000, # 1_000\n",
        ")\n",
        "\n",
        "agent_2.train(episodes=1_000,\n",
        "            episode_duration=70,\n",
        "            evaluation_freq=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TimeToCollisionVec' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ttc_vec_3 \u001b[38;5;241m=\u001b[39m \u001b[43mTimeToCollisionVec\u001b[49m(num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      2\u001b[0m                             horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      3\u001b[0m                             policy_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      4\u001b[0m                             simulation_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m                             collision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m      6\u001b[0m                             high_speed_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m      7\u001b[0m                             right_lane_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      8\u001b[0m                             lane_change_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      9\u001b[0m                             obs_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeed-restricted\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m                             string_shaped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m agent_3 \u001b[38;5;241m=\u001b[39m DQNAgent(ttc_vec_3, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN_2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     13\u001b[0m agent_3\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m     14\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     15\u001b[0m     min_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     min_replay_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000\u001b[39m, \u001b[38;5;66;03m# 1_000\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'TimeToCollisionVec' is not defined"
          ]
        }
      ],
      "source": [
        "ttc_vec_3 = TimeToCollisionVec(num_envs=10,\n",
        "                            horizon=5,\n",
        "                            policy_frequency=3,\n",
        "                            simulation_frequency=20,\n",
        "                            collision_reward=-200,\n",
        "                            high_speed_reward=12,\n",
        "                            right_lane_reward=10,\n",
        "                            lane_change_reward=5,\n",
        "                            obs_type='speed-restricted',\n",
        "                            string_shaped=True)\n",
        "\n",
        "agent_3 = DQNAgent(ttc_vec_3, model_type='NN_2')\n",
        "agent_3.init(\n",
        "    epsilon=1,\n",
        "    min_eps=0.01,\n",
        "    eps_decay=0.996,  #.998\n",
        "    gamma=0.995,\n",
        "    alpha=0.15,\n",
        "    replay_memory=50_000,  #50_000\n",
        "    batch_size=32,   # 64\n",
        "    train_model_freq=5,\n",
        "    update_weights_freq=100,\n",
        "    min_replay_memory=1_000, # 1_000\n",
        ")\n",
        "\n",
        "agent_3.train(episodes=1_000,\n",
        "            episode_duration=30,\n",
        "            evaluation_freq=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TimeToCollisionVec' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ttv_vec_4 \u001b[38;5;241m=\u001b[39m \u001b[43mTimeToCollisionVec\u001b[49m(num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      2\u001b[0m                             horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m      3\u001b[0m                             policy_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      4\u001b[0m                             simulation_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m                             collision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m      6\u001b[0m                             high_speed_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m      7\u001b[0m                             right_lane_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,\n\u001b[0;32m      8\u001b[0m                             lane_change_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      9\u001b[0m                             obs_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeed-restricted\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m                             string_shaped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m agent_4 \u001b[38;5;241m=\u001b[39m DQNAgent(ttv_vec_4, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN_2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m agent_4\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m     15\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     16\u001b[0m     min_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     min_replay_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000\u001b[39m, \u001b[38;5;66;03m# 1_000\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'TimeToCollisionVec' is not defined"
          ]
        }
      ],
      "source": [
        "ttv_vec_4 = TimeToCollisionVec(num_envs=10,\n",
        "                            horizon=5,\n",
        "                            policy_frequency=4,\n",
        "                            simulation_frequency=20,\n",
        "                            collision_reward=-100,\n",
        "                            high_speed_reward=12,\n",
        "                            right_lane_reward=12,\n",
        "                            lane_change_reward=4,\n",
        "                            obs_type='speed-restricted',\n",
        "                            string_shaped=True)\n",
        "\n",
        "agent_4 = DQNAgent(ttv_vec_4, model_type='NN_2')\n",
        "\n",
        "agent_4.init(\n",
        "    epsilon=1,\n",
        "    min_eps=0.01,\n",
        "    eps_decay=0.996,  #.998\n",
        "    gamma=0.995,\n",
        "    alpha=0.05,\n",
        "    replay_memory=50_000,  #50_000\n",
        "    batch_size=128,   # 64\n",
        "    train_model_freq=6,\n",
        "    update_weights_freq=300,\n",
        "    min_replay_memory=1_000, # 1_000\n",
        ")\n",
        "\n",
        "agent_4.train(episodes=1_000,\n",
        "            episode_duration=40,\n",
        "            evaluation_freq=10, \n",
        "            tolerance=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TimeToCollisionVec' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ttc_vec_5 \u001b[38;5;241m=\u001b[39m \u001b[43mTimeToCollisionVec\u001b[49m(num_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      2\u001b[0m                             horizon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m      3\u001b[0m                             policy_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      4\u001b[0m                             simulation_frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m                             collision_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m120\u001b[39m,\n\u001b[0;32m      6\u001b[0m                             high_speed_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m      7\u001b[0m                             right_lane_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m      8\u001b[0m                             lane_change_reward\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      9\u001b[0m                             obs_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeed-restricted\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m                             string_shaped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m agent_5 \u001b[38;5;241m=\u001b[39m DQNAgent(ttc_vec_5, model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m agent_5\u001b[38;5;241m.\u001b[39minit(\n\u001b[0;32m     15\u001b[0m     epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     16\u001b[0m     min_eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     min_replay_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1_000\u001b[39m, \u001b[38;5;66;03m# 1_000\u001b[39;00m\n\u001b[0;32m     25\u001b[0m )\n",
            "\u001b[1;31mNameError\u001b[0m: name 'TimeToCollisionVec' is not defined"
          ]
        }
      ],
      "source": [
        "ttc_vec_5 = TimeToCollisionVec(num_envs=10,\n",
        "                            horizon=4,\n",
        "                            policy_frequency=3,\n",
        "                            simulation_frequency=20,\n",
        "                            collision_reward=-120,\n",
        "                            high_speed_reward=10,\n",
        "                            right_lane_reward=8,\n",
        "                            lane_change_reward=3,\n",
        "                            obs_type='speed-restricted',\n",
        "                            string_shaped=True)\n",
        "\n",
        "agent_5 = DQNAgent(ttc_vec_5, model_type='NN_1')\n",
        "\n",
        "agent_5.init(\n",
        "    epsilon=1,\n",
        "    min_eps=0.01,\n",
        "    eps_decay=0.996,  #.998\n",
        "    gamma=0.995,\n",
        "    alpha=0.025,\n",
        "    replay_memory=50_000,  #50_000\n",
        "    batch_size=64,   # 64\n",
        "    train_model_freq=2,\n",
        "    update_weights_freq=150,\n",
        "    min_replay_memory=1_000, # 1_000\n",
        ")\n",
        "\n",
        "agent_5.train(episodes=1_000,\n",
        "            episode_duration=15,\n",
        "            evaluation_freq=10, \n",
        "            tolerance=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent.evaluate_and_render(n_games=3, steps=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "________________________________________"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8BhZl5k4MNb4",
        "outputId": "25f13f03-5873-45e3-ca66-583d171b2920"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"observation\": {\n",
        "        \"type\": \"TimeToCollision\"\n",
        "    },\n",
        "    \"vehicles_count\": 50,\n",
        "    \"duration\": 120,\n",
        "    \"policy_frequency\": 5,\n",
        "    \"simulation_frequency\": 15\n",
        "}\n",
        "env = gym.make(\"highway-fast-v0\")\n",
        "env.configure(config)\n",
        "env.reset()\n",
        "\n",
        "model = DQN(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    learning_rate=1e-3,\n",
        "    buffer_size=50000,\n",
        "    learning_starts=1000,\n",
        "    batch_size=32,\n",
        "    tau=1.0,\n",
        "    gamma=0.99,\n",
        "    train_freq=4,\n",
        "    gradient_steps=1,\n",
        "    target_update_interval=1000,\n",
        "    exploration_fraction=0.1,\n",
        "    exploration_final_eps=0.02,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Definieren Sie einen Checkpoint-Callback\n",
        "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path='./models/', name_prefix='dqn_model')\n",
        "\n",
        "# Trainieren Sie das Modell\n",
        "model.learn(total_timesteps=100000, callback=checkpoint_callback)\n",
        "\n",
        "# Speichern Sie das Modell\n",
        "model.save(\"dqn_highway\")\n",
        "\n",
        "# Laden Sie das Modell\n",
        "model = DQN.load(\"dqn_highway\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "kVoc_0EPRRf2",
        "outputId": "ffb5e4c8-0054-4601-b71b-6e2e75b4dc2d"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"highway-fast-v0\", render_mode=\"human\", config=config)\n",
        "\n",
        "while True:\n",
        "  done = truncated = False\n",
        "  obs, info = env.reset()\n",
        "  while not (done or truncated):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    env.render()\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00787b19dd584aaa84c59ffdd727705d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bb6d3ca6796494c8b8b94b3f691325c",
            "placeholder": "​",
            "style": "IPY_MODEL_c8619f90e4e24dc2892446a079df904a",
            "value": "  0%"
          }
        },
        "0569add33f8b4f2385d269eb9b3c9e0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b16fb7a61964379a0047c95fbadfd0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ba00075524c4e2aa7fce4389baade81",
            "placeholder": "​",
            "style": "IPY_MODEL_508034f94a54419293b756a277324a85",
            "value": " 8/2001 [03:01&lt;9:12:37, 16.64s/episode]"
          }
        },
        "508034f94a54419293b756a277324a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50f0cc04c9f2427bb5c13c97537b873e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d3ad70cae4f4dc9ab4df360ea0a98ee",
            "max": 2001,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5085c2eecb54a44b9693e9fc449fbd2",
            "value": 8
          }
        },
        "8bb6d3ca6796494c8b8b94b3f691325c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d3ad70cae4f4dc9ab4df360ea0a98ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ba00075524c4e2aa7fce4389baade81": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5085c2eecb54a44b9693e9fc449fbd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bfd047eee07a4f399c951c390f46d41e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00787b19dd584aaa84c59ffdd727705d",
              "IPY_MODEL_50f0cc04c9f2427bb5c13c97537b873e",
              "IPY_MODEL_3b16fb7a61964379a0047c95fbadfd0f"
            ],
            "layout": "IPY_MODEL_0569add33f8b4f2385d269eb9b3c9e0c"
          }
        },
        "c8619f90e4e24dc2892446a079df904a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
